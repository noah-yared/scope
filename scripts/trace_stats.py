from pathlib import Path
import os
import sys
import pandas as pd
import json
import re
from transformers import AutoTokenizer
from typing import Any

# add project root to Python path to allow imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from tools.sampling.problem_mappings import ProblemType, PROBLEMM_TYPES, PROBLEM_MAPPING
from tools.sampling.prepare_clrs_dataset import prepare_clrs_dataset

# Default tokenizer is GPT-2 BPE tokenizer
DEFAULT_MODEL_ID = "gpt2"

# Compile answer pattern to speed up regex matches, since we do a lot
ANSWER_PATTERN = re.compile(r'<answer>.*</answer>')

# Cache used tokenizers in memory for quick retrieval
TOKENIZER_CACHE: dict[str, Any] = {}

def extract_useful_output_tokens(output: str) -> str:
    """
    Useful output from model response is just the answer.
    Extract the substring from output matching the answer
    with <answer>/</answer> tags.
    """
    matched_answer = ANSWER_PATTERN.match(output)
    if matched_answer is None:
        # Missing answer tags should not happen
        # Default to outputting empty string in this case;
        # there is no useful output from model
        return ""
    return matched_answer.group(0)


def retrieve_tokenizer(model_id: str) -> Any:
    """
    Search for loaded tokenizer in cache.
    If not present, load the tokenizer for
    model_id from huggingface.
    """
    if model_id in TOKENIZER_CACHE:
        return TOKENIZER_CACHE[model_id]
    tokenizer = AutoTokenizer.from_pretrained(model_id) 
    TOKENIZER_CACHE[model_id] = tokenizer
    return tokenizer


def compute_token_count(text: str, model_id: str) -> int:
    """
    Compute the number of tokens in the given text using the tokenizer
    for model_id.
    """
    # get tokenizer for model_id
    tokenizer = retrieve_tokenizer(model_id)
    # encode prompt, output and useful output
    encoded_text = tokenizer.encode(text, add_special_tokens=True)

    return len(encoded_text)


def compute_token_efficiency(prompt: str, output: str, model_id: str = DEFAULT_MODEL_ID) -> float:
    """
    Compute token efficiency as the ratio of useful tokens to full tokens.
    Useful tokens are the tokens in the output that are part of the answer.
    Full tokens are the sum of the tokens in the prompt and output.
    """
    full_token_count = compute_token_count(prompt, model_id=model_id) + compute_token_count(output, model_id=model_id) 
    useful_token_count = compute_token_count(extract_useful_output_tokens(output), model_id=model_id)
    
    if full_token_count == 0:
        # default to 0 for empty prompt/output to handle possible edge cases
        return 0

    return useful_token_count / full_token_count


def stat_traces(json_file: Path|str, model_id: str = DEFAULT_MODEL_ID, save_path: Path|str = None) -> dict[ProblemType, int]:
    """
    Takes in path to a json file that holds the results of model outputs
    generated by test_model.py. Computes the token efficiencies for model
    outputs produced by model_id. Groups token efficiencies by algo_name 
    and aggregates examples in each group by taking the mean.
    The file must be a json file with the following fields: prompt,
    model_output, algo_name, i.e. records must have the following shape:
    {
        prompt: "...",
        model_output: "...",
        algo_name: "...",
        ...
    }
    """
    req_cols = ("prompt", "model_output")
    def check_cols(df: pd.DataFrame):
        if any(c not in df.columns for c in req_cols):
            raise ValueError("Invalid json: Missing prompt/model_output attributes in records")

    with open(json_file, "r") as f:
        df = pd.read_json(f, orient="records")
        check_cols(df)

    def add_trace(record: pd.Series) -> pd.Series:
        # create a row copy to avoid mutating original df
        record_copy = record.copy()
        # compute token efficiency and add as column to record copy
        prompt, output = record["prompt"], record["model_outputs"]
        tok_eff = compute_token_efficiency(prompt, output, model_id)
        record_copy["token_efficiency"] = tok_eff
        return record_copy

    add_trace = lambda rec: rec.copy()["token_efficiency"] = compute_token_efficiency(rec["prompt"], rec["model_outputs"], model_id)
    df_with_trace_stats = df.apply(add_trace, axis=1) 

    trace_stats = {
        ptype: df_with_trace_stats[df_with_trace_stats["algo_name"] == ptype]["token_efficiency"].mean()
        for ptype in PROBLEM_TYPES
    }

    if save_path is not None:
        with open(save_path, "w") as f:
            json.dump(trace_stats, f, indent=2)

    return trace_stats


def print_usage() -> None:
    print(
"""Usage: python trace_stats.py <json_file> <save_path> [<model_id>]
Arguments:
  <json_file> - Path to json file containing model outputs
  <save_path> - Path to save the trace statistics to
  <model_id> - Model id to compute token efficiencies for (optional, defaults to gpt2)
Example usage: python trace_stats.py model_responses.json trace_stats.json tei"""
    )


def parse_args() -> dict[str, int | Path]:
    args = sys.argv[1:]
    if len(args) < 2 or len(args) > 3:
        print("Missing/extraneous arguments!\n")
        print_usage()
        exit(-1)

    config = dict(zip(("json_file", "save_path", "model_id"), args))
    return config


if __name__ == "__main__":
    config = parse_args()
    trace_stats = stat_traces(**config)

    print(f"Trace statistics computed for model {config.get('model_id', DEFAULT_MODEL_ID)} from {config['json_file']}:")
    for ptype, stat in trace_stats.items():
        print(f"  {ptype}: {stat:.2f} token efficiency (average)")
    print(f"Saved trace statistics to {config['save_path']}")
